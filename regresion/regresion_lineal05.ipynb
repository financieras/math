{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKmOo0pAt4y3LpSBJFear0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/math/blob/main/regresion/regresion_lineal05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co9nJVNLXgku",
        "outputId": "bf924556-18a7-407b-caeb-4150480785c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergencia alcanzada en la iteración 605\n",
            "Intercepto (theta0): 4.3000\n",
            "Pendiente (theta1): 2.3000\n",
            "Error final: 0.034672\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def linear_regression_gradient_descent(X, Y, learning_rate=0.01, num_iterations=1000):\n",
        "    \"\"\"\n",
        "    Implementa regresión lineal simple usando descenso del gradiente.\n",
        "\n",
        "    Parámetros:\n",
        "    X: ndarray - Variable independiente (kilometraje)\n",
        "    Y: ndarray - Variable dependiente (precio)\n",
        "    learning_rate: float - Tasa de aprendizaje\n",
        "    num_iterations: int - Número de iteraciones\n",
        "\n",
        "    Retorna:\n",
        "    theta0: float - Intercepto\n",
        "    theta1: float - Pendiente\n",
        "    history: dict - Historial de pérdida y parámetros\n",
        "    \"\"\"\n",
        "    # Normalización\n",
        "    X_mean, X_std = np.mean(X), np.std(X)\n",
        "    Y_mean, Y_std = np.mean(Y), np.std(Y)\n",
        "\n",
        "    X_norm = (X - X_mean) / X_std\n",
        "    Y_norm = (Y - Y_mean) / Y_std\n",
        "\n",
        "    # Inicialización\n",
        "    theta1_norm = 0\n",
        "    theta0_norm = 0\n",
        "    m = len(X)  # número de muestras\n",
        "\n",
        "    # Para almacenar el historial\n",
        "    history = {\n",
        "        'loss': [],\n",
        "        'theta0_norm': [],\n",
        "        'theta1_norm': []\n",
        "    }\n",
        "\n",
        "    # Descenso del gradiente\n",
        "    for i in range(num_iterations):\n",
        "        # Predicciones\n",
        "        Y_pred_norm = theta1_norm * X_norm + theta0_norm\n",
        "\n",
        "        # Error cuadrático medio actual\n",
        "        current_loss = np.mean((Y_norm - Y_pred_norm) ** 2)\n",
        "\n",
        "        # Gradientes (tu signo estaba invertido)\n",
        "        D_theta1_norm = (2/m) * np.sum(X_norm * (Y_pred_norm - Y_norm))\n",
        "        D_theta0_norm = (2/m) * np.sum(Y_pred_norm - Y_norm)\n",
        "\n",
        "        # Actualización de parámetros\n",
        "        theta1_norm -= learning_rate * D_theta1_norm\n",
        "        theta0_norm -= learning_rate * D_theta0_norm\n",
        "\n",
        "        # Guardar historial\n",
        "        history['loss'].append(current_loss)\n",
        "        history['theta0_norm'].append(theta0_norm)\n",
        "        history['theta1_norm'].append(theta1_norm)\n",
        "\n",
        "        # Criterio de convergencia temprana (opcional)\n",
        "        if i > 0 and abs(history['loss'][-1] - history['loss'][-2]) < 1e-12:\n",
        "            print(f\"Convergencia alcanzada en la iteración {i}\")\n",
        "            break\n",
        "\n",
        "    # Desnormalización\n",
        "    theta1 = theta1_norm * (Y_std / X_std)\n",
        "    theta0 = Y_mean - theta1 * X_mean + theta0_norm * Y_std\n",
        "\n",
        "    return theta0, theta1, history\n",
        "\n",
        "# Ejemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Datos de ejemplo\n",
        "    X = np.array([1, 2, 3, 4, 5])\n",
        "    Y = np.array([7, 8, 12, 13, 16])\n",
        "\n",
        "    # Ejecutar regresión\n",
        "    theta0, theta1, history = linear_regression_gradient_descent(X, Y)\n",
        "\n",
        "    print(f\"Intercepto (theta0): {theta0:.4f}\")\n",
        "    print(f\"Pendiente (theta1): {theta1:.4f}\")\n",
        "    print(f\"Error final: {history['loss'][-1]:.6f}\")\n"
      ]
    }
  ]
}